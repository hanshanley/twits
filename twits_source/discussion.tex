\section{Discussion}
In this work, we determined the correlation of different aspects of partisanship and affective polarization with toxicity at a user and topic-level on Twitter. We find, most notably, that users who are at the tail end of the political spectrum (very right-leaning or very left-leaning) \emph{are not} more likely to post toxic content; rather, we observe that users that engage with a wide variety of different politically aligned accounts center have a higher likelihood of tweeting toxic messages. Further, as users interact with or mention other users from a wider range of political ideologies, they are more likely to post toxic content. We similarly find that users who interact with other users who more regularly post toxic content are more likely to post toxic content themselves. 

Examining these phenomena from a topic level, we find that most heavily partisan topics \emph{are not} the most toxic. Rather, topics often have complex relationships with the partisanship of the users who tweet about them. While some topics become more toxic as more right-leaning/left-leaning users tweet about them, others become less toxic. However, as with individual users, we find that as users from a wider range of political ideologies tweet about a given topic, the more toxic that topic. Here we discuss some of the limitations and implications of our results:


\subsection{Limitations}
We acknowledge several limitations of this work. Given our use of GAMs to estimate the effect of partisanship and political diversity and our lack of ability to perform direct experiments, our findings are largely correlational. While they do buttress and support a large literature of similar results~\cite{barbera2014social,barbera2015tweeting,dagoula2019mapping,bail2018exposure} that have found causal results in some cases for increased polarization due to interaction with users of different political beliefs, we acknowledge that \emph{our} results are not causal. We further note that due to new restrictions placed on the collection of Tweets~\cite{Singh2023}, we can not continue to measure the toxicity of users and political topics, going forward.

This work largely focuses on US-based political polarization and ideologies. As a result, while applicable to dynamics for Twitter accounts on the US-political spectrum, our results do not necessarily apply to political conversations in different contexts. However, given access to Twitter or similar social media websites such as Meta's Threads, our study can be replicated in different cultural contexts.

Finally, as found early in our work in Section~\ref{sec:labeling-toxic}, different individuals and datasets have different metrics for toxicity. While our use of Perspective API's definition of toxicity is standard throughout the literature~\cite{kumar2021designing,rajadesingan2020quick,saveski2021structure}, we do base our DeBERTa-based model toxicity detection on this definition; we acknowledge that it may not take into account all perspectives on what constitutes toxic online content. 


\subsection{Tribal Tendencies, Affective Polarization, Online Toxicity, and Online Echo Chambers on Twitter}
As found by others, heated political conversations often elicit toxicity as people of differing views debate and discuss their differences~\cite{salminen2020topic}. We find that this discourse is related to increased toxicity on Twitter. The political diversity of those involved in a given Twitter conversation surrounding a given topic, at least in the short form of tweets, is correlated with affective polarization and toxic content. Adding nuance to previous studies of communities that have found that like-minded users gather and reinforce each other's views, creating toxic echo-chambers~\cite{cinelli2020echo,starbird2018ecosystem,gronlund2015does}. While users naturally often congregate and more heavily engage with users like themselves (assortativity coefficient of 0.266), showing that some echo chambers may exist on Twitter, when users exit these chambers and engage with other users of differing political views, we observe that this tends to create user conflict~\cite{guess2018avoiding}. This result reinforces De Francisci Morales {et~al.}'s~\cite{de2021no} finding that interactions among users on Reddit with different political orientations have increased negative conversational outcomes, showing that it occurs in platform-wide user interactions and discussions. Further indeed across all users, we find that as they increasingly interact with users of different partisanship, the frequency of toxicity increases (Figure~\ref{fig:toxicity_vs_mention-new}). While this feature of online conversation is not the dominant factor in engendering toxic content, with other factors like a user's previous behavior~\cite{levy2022understanding}, the age of their account, and the toxicity of other users also contributing to online toxicity, we note that this apparent ``tribal tendency'' appears both on a user and topic-level across Twitter and across multiple Twitter threads illustrating the robustness of this finding~\cite{mamakos2023social,de2021no}.  

%Namely, as also found by Mamakos et al.~\cite{mamakos2023social}, as users engage with users different from them and in a wider variety of political contexts, they tend to be more toxic. We further note that this result also reinforces De Francisci Morales {et~al.}'s~\cite{de2021no} finding that interactions among users on Reddit with different political orientations have increased negative conversational outcomes. 


\subsection{Hyperpartisan Users and Topics}
In contrast to some prior work~\cite{nelimarkka2018social},  we find that users and topics that are hyperpartisan (\textit{i.e.}, very left-leaning users or very right-leaning users) are not necessarily more toxic than less ideological users. Rather, we find these users tend to mostly associate and interact with other users who share similar political views ($\rho=0.605$) and as a result, do not necessarily have higher toxicity levels. As also found by Gr{\"o}nlund et al.~\cite{gronlund2015does}, because hyperpartisan users and topics often do not attract users of differing political views, we find that these users and topics tend to be less toxic than topics and users that interact with a wider range of the political spectrum (\textit{e.g.}, topics and users nearer to the political center). This result indicates that political echo chambers, where only left-leaning or right-leaning interact among themselves may be less conflict-oriented on Twitter. As a result, we argue that if social media companies like Twitter wish to expose their users to a wider range of political views without increasing conflict on their platforms, these users may be amenable to these opposing views if they come from others nearer to themselves on the political spectrum.


%\subsection{Toxicity } While our work does find evidence of th  Our work adds to the growing literature challenging the prevalence 

\subsection{Intra-Topic Partisanship over Time}
In Section~\ref{sec:changes}, we observed that the political orientation of users who discuss any particular topic often changes over time. These changes, often coinciding with changes in toxicity, also illustrate that the views expressed on Twitter about particular topics often change as different users enter or leave conversations. We argue that future analysis of topics and their spread on Twitter \emph{must} take into account user-level characteristics such as partisanship given that these values often reveal the nature of how users are addressing individual topics. For example, as seen in Section~\ref{sec:changes}, understanding that conversations surrounding ``Moscow Mitch'' had been taken up by increasingly right-leaning users reveals the penetration of this insult into more conservative circles. 

%\subsection{Verified Users Contribution to Toxicity}
%While we find that users who are verified are less likely to be toxic in their interactions on Twitter, we find conversely that as more verified users are involved in a particular topic, this coincides with the toxicity of that topic increasing. We hypothesize that topics, where many verified users are involved, are fairly high-profile attracting users from across the political spectrum and thus encouraging increased affective polarization. We leave it to future work to fully examine these dynamics. 




\subsection{Toxic Birds of Feather}
In addition to finding that the range of political views encountered by a particular user is predictive of toxicity, we further find that topics and users who interact with other toxic users are more likely to be toxic themselves. This again buttresses prior work from Kim {et~al.}, Kwon {et~al.}, and Shen {et~al.}  who all find that exposure to these negative conversations actually increases observers' tendency to also engage in incivility~\cite{kim2019incivility,kwon2017offensive,shen2020viral}. While not a new finding~\cite{kumar2023understanding}, this illustrates that reducing toxic content online may have other downstream benefits; by removing more instances of toxic content, other users may be less likely to engage in toxicity themselves further reducing the amount of toxic content. Given the existence of particular toxicity norms within communities Reddit~\cite{rajadesingan2020quick}, where toxicity is rarely seen among users and toxic comments are looked down upon, we argue that removing toxic content may have a compounding effect, greatly improving the overall health of online discourse.




\subsection{Implications for the Twitter/X Platform} Our work simultaneously finds that topics that engage with a wider set of politically aligned users and that users that engage in a wider array of different political discussions are more likely to tweet toxic messages. Namely, exposure on the Twitter/X platform to differing views may essentially be counterproductive to producing civil online discussions~\cite{bail2018exposure}.  Furthermore, this suggests that recent attempts to widen the range of political discussion on Twitter may have the additional effect of increasing online toxicity~\cite{hickey2023auditing}. As such, we argue that as Twitter continues to widen the political conversation on its platform, to also maintain low levels of toxicity additional moderation steps or additional practices should be taken to slowly introduce users to other accounts with different political beliefs to themselves should be taken as well~\cite{munson2010presenting}. This accords with the recommendations and findings of Mamakos et al.~\cite{mamakos2023social} who found that as Reddit users engage with users different from them and in a wider variety of political contexts, they tend to be more toxic. Given that Twitter users are not siphoned out into individual communities that they specifically join and thus more easily engage with polarizing content and users with whom they disagree across their topics of interest, we argue that building a means by which to engage in better conversations across political differences can reduce toxicity and friction on the platform. For example, as also argued by~\cite{nelimarkka2018social} including a wide and generalized view of particular topics could potentially reduce polarization. Indeed, as found in Section~\ref{sec:topic-level-gam}, while initially sparking more toxicity, as topics include a wider and wider berth of political perspectives and as more users join a topic, the toxicity of that particular topic decreases. 

%We further note that this result also reinforces De Francisci Morales {et~al.}'s~\cite{de2021no} finding that interactions among users on Reddit with different political orientations have increased negative conversational outcomes. 

%We similarly find several instances where users will simply @/mention verified users and begin toxicly tweeting at them. Given the high degree of these types of ``toxicity waves'' and that these waves largely seem to focus on verified users, we argue that Twitter/X should take additional steps to protect these users or alert them when such a toxicity wave is occurring. 



\subsection{Future Work}
This work centered around understanding factors that contribute to the toxicity levels of individual users and within particular topics on Twitter/X. However, we note that several of the techniques employed within this work can be extended and utilized beyond our study.

%\vspace{2pt}
%\noindent
%\textbf{Identifying Hate and Toxic Topic Waves} As seen in Section~\ref{sec:partisan-toxic}, utilizing our approach, we managed to identify various instances where accounts encountered toxic tweets aimed at them and centered around a particular topic. For instance, we identified 16 different waves aimed at Elon Musk; altogether we identified 1,923 similar ``toxicity waves'' aimed at 3,822 different accounts. We note that our DeBERTa-based model, which we open-source for study and use, can further enable others to continue this work without having to rely on making online and black-box queries to the Perspective API. In future work, we plan to better identify when users specifically attack particular accounts by training a model to predict the ``ATTACK ON AUTHOR'' task provided by Google Jigsaw~\cite{perspectiveapi} along with other metadata embedded within Twitter conversations. While not the focus on this particular work, we note that by being able to automatically identify potential ``toxicity waves'' against particular users, our work could be utilized to protect journalists, public officials, and vulnerable populations.

\vspace{2pt}
\noindent
\textbf{Identifying the Role of Partisanship and Polarization on Different Platforms} In this work, while we focus on Twitter, we note that our approach can largely be utilized on different social media platforms (\textit{e.g.}, Facebook, Reddit, \textit{etc...}) to identify the role of partisanship and political polarization. Unlike on Twitter, where a feed is curated for the user, Reddit user interactions, for instance, are largely determined by the into which the communities self-select. Previous work has shown that entire communities can engage in cross-partisan toxic behavior~\cite{efstratiou2022non}. Similarly Bail et~al.~\cite{bail2018exposure} find that simply following users and repeatedly seeing disagreeable content can increase polarization. As such, we plan to explore the robustness of our findings about ``tribal tendencies'' in different contexts and what best practices can be utilized to ameliorate these tendencies. 









